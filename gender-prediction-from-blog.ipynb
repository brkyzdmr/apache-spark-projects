{"cells":[{"cell_type":"markdown","source":["# Can you predict the gender from an author's blog post?\n\nThe objective of this notebook is to demonstrate a simple end-to-end pipeline for using Spark-ML. I have chosen the Blog Authorship Corpus in this project. It is available at http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm. The script for parsing the blog posts (in XML format) into a JSON dataset is provided alongside this project."],"metadata":{}},{"cell_type":"code","source":["# Let's load all the dependencies\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import StringIndexer, Tokenizer, HashingTF, StopWordsRemover, RegexTokenizer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Load the dataset into a dataframe. The JSON file containing the dataset was uploaded to DataBricks's FileStore and a table was created.\ndf = spark.sql(\"SELECT * FROM author_blogs_dataset_mini_json\").select(\"text\",\"gender\")\ndf.cache()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["display(df.take(10))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Let's see the class distribution in our dataset\ndf.groupBy('gender').count().show()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["#simple preprocessing pipeline\n\nimport re\n\ndef preprocess(text):\n    \n    text = text.replace('\\\\r\\\\n',' ')\n    text = text.replace('\\\\n\\\\t',' ')\n    text = text.replace('\\\\t',' ')\n    text = re.sub('&nbsp',' ',text)\n    text = re.sub('\\n',' ',text)\n    text = re.sub('[^\\w+|\\s]',' ',text)\n    text = text.lower()\n    return text"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# create an user defined function (udf)\n\nfrom pyspark.sql.types import StringType\ntext_clean_udf = udf(preprocess, StringType())"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# add the cleaned column to the dataset based on the udf\ndf_cleaned = df.withColumn('text_cleaned',text_clean_udf('text'))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# original\n(df_cleaned\n .select('text')\n .show(1,truncate=False))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# preprocessed\n(df_cleaned\n .select('text_cleaned')\n .show(1,truncate=False))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["#Define feature-creation pipeline\nlabelIndexer = StringIndexer(inputCol=\"gender\", outputCol=\"label\", handleInvalid=\"keep\")\nregexTokenizer = RegexTokenizer(inputCol=\"text_cleaned\", outputCol=\"words\",pattern='\\s+|[^a-zA-z]')\nstopwordsRemover = StopWordsRemover(inputCol=\"words\",outputCol=\"filtered\")\nhashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"features\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["#build a pipeline using the components above.\npipeline = Pipeline(stages=[labelIndexer, regexTokenizer,stopwordsRemover,hashingTF])"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["#define a Logistic Regression Model\nlr = LogisticRegression(maxIter=20)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Fit the pipeline to the dataset\ntransformerModel = pipeline.fit(df_cleaned)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["#show transformed dataset\ndisplay(transformerModel.transform(df_cleaned).take(3))"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["#extract only features and label from the transformed dataset\ndataset = transformerModel.transform(df_cleaned).select('label','features')"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["display(dataset.take(3))"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# take a stratified sample as training data, we want a 80-20 split between training and test data.\ntrain_data = dataset.sampleBy('label',fractions={0:0.8,1:0.8})"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["train_data.groupBy('label').count().show()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["#the remaining is hold-out for testing our model's performance\ntest_data = dataset.subtract(train_data)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["test_data.groupBy('label').count().show()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# build our model by fitting on the training data\nmodel = lr.fit(train_data)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# get the predictions\nresults = model.transform(test_data)\ndisplay(results)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n#instantiate Evaluator object, default metric is areaUnderROC\nevaluator = BinaryClassificationEvaluator()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["evaluator.evaluate(results)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["areaUnderROC is 0.59. So our model is slightly better than random prediction (ROC = 0.5)\n\nThe objective of this notebook was to show an end-to-end pipeline for loading & preprocessing the dataset, building a model and evaluating it. We'll investigate\ntuning of the algorithm in next posts.\n\n\nThank you. :)"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":28}],"metadata":{"name":"gender-prediction-from-blog","notebookId":4217047351159525},"nbformat":4,"nbformat_minor":0}
